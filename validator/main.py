import os
from typing import Callable, Dict, Optional

from guardrails.validator_base import (
    FailResult,
    PassResult,
    ValidationResult,
    Validator,
    register_validator,
)
from guardrails.stores.context import get_call_kwarg
from litellm import completion, get_llm_provider


@register_validator(name="arize/relevancy_evaluator", data_type="string")
class RelevancyEvaluator(Validator):
    """This class validates an output generated by a LiteLLM (LLM) model by prompting another LLM model to evaluate the output.

    **Key Properties**

    | Property                      | Description                       |
    | ----------------------------- | --------------------------------- |
    | Name for `format` attribute   | `arize/relevancy_evaluator`       |
    | Supported data types          | `string`                          |
    | Programmatic fix              | N/A                               |

    Args:
        llm_callable (str, optional): The name of the LiteLLM model to use for validation. Defaults to "gpt-3.5-turbo".
        on_fail (Callable, optional): A function to be called when validation fails. Defaults to None.
    """

    def __init__(
        self,
        llm_callable: str = "gpt-3.5-turbo",  # str for litellm model name
        on_fail: Optional[Callable] = None,
        **kwargs,
    ):
        super().__init__(on_fail, llm_callable=llm_callable, **kwargs)
        self.llm_callable = llm_callable

    def get_validation_prompt(self, original_prompt: str, reference: str) -> str:
        """Generates the prompt to send to the LLM.

        Args:
            value (str): The value to validate.
            question (str): The question to ask the LLM.

        Returns:
            prompt (str): The prompt to send to the LLM.
        """
        prompt = f"""
        You are comparing a reference text to a question and trying to determine if the reference text
        contains information relevant to answering the question. Here is the data:
            [BEGIN DATA]
            ************
            [Question]: {original_prompt}
            ************
            [Reference text]: {reference}
            ************
            [END DATA]
        Compare the Question above to the Reference text. You must determine whether the Reference text
        contains information that can answer the Question. Please focus on whether the very specific
        question can be answered by the information in the Reference text.
        Your response must be single word, either "relevant" or "unrelated",
        and should not contain any text or characters aside from that word.
        "unrelated" means that the reference text does not contain an answer to the Question.
        "relevant" means the reference text contains an answer to the Question.        
        """
        return prompt

    def get_llm_response(self, prompt: str) -> str:
        """Gets the response from the LLM.

        Args:
            prompt (str): The prompt to send to the LLM.

        Returns:
            str: The response from the LLM.
        """
        # 0. Create messages
        messages = [{"content": prompt, "role": "user"}]
        
        # 0b. Setup auth kwargs if the model is from OpenAI
        kwargs = {}
        _model, provider, *_rest = get_llm_provider(self.llm_callable)
        if provider == "openai":
            kwargs["api_key"] = get_call_kwarg("api_key") or os.environ.get("OPENAI_API_KEY")

        # 1. Get LLM response
        # Strip whitespace and convert to lowercase
        try:
            response = completion(model=self.llm_callable, messages=messages, **kwargs)
            response = response.choices[0].message.content  # type: ignore
            response = response.strip().lower() # type: ignore
        except Exception as e:
            raise RuntimeError(f"Error getting response from the LLM: {e}") from e

        # 3. Return the response
        return response

    def validate(self, value: str, metadata: Dict) -> ValidationResult:
        """
        Validates is based on the relevance of the reference text to the original question.

        Args:
            value (str): The value to validate.
            metadata (Dict): The metadata for the validation. It must contain the key 'original_prompt', 
            with the original question that the reference text is being compared to.

        Returns:
            ValidationResult: The result of the validation. It can be a PassResult if the reference 
                              text is relevant to the original question, or a FailResult otherwise.
        """
        # 1. Get the question and arg from the value
        original_prompt = metadata.get("original_prompt")
        if original_prompt is None:
            raise RuntimeError(
                "original_prompt missing from metadata. "
                "Please provide the original prompt."
            )

        reference = value
        if reference is None:
            raise RuntimeError(
                "Please pass a non-None value. "
                "Please provide the reference text."
            )

        # 2. Setup the prompt
        prompt = self.get_validation_prompt(original_prompt, reference)

        # 3. Get the LLM response
        llm_response = self.get_llm_response(prompt)

        # 4. Check the LLM response and return the result
        if llm_response == "unrelated":
            return FailResult(error_message="The LLM says 'unrelated'. The validation failed.")

        if llm_response == "relevant":
            return PassResult()

        return FailResult(
            error_message="The LLM returned an invalid answer. Failing the validation..."
        )
